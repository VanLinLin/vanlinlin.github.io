from scholarly import scholarly
import json
from datetime import datetime
import os

# --- æ ¸å¿ƒä¿®æ­£ ---
# ç²å–ç•¶å‰è…³æœ¬ (main.py) æ‰€åœ¨çš„ç›®éŒ„
script_dir = os.path.dirname(os.path.abspath(__file__))
# çµ„åˆå‡ºæ­£ç¢ºçš„ results è³‡æ–™å¤¾è·¯å¾‘
results_dir = os.path.join(script_dir, 'results')
# -----------------

try:
    print("ğŸš€ Starting scholar crawler")

    # åŸ·è¡Œçˆ¬èŸ²
    author: dict = scholarly.search_author_id('ook-p6wAAAAJ&hl')
    scholarly.fill(author, sections=['basics', 'indices', 'counts', 'publications'])
    name = author['name']
    author['updated'] = str(datetime.now())
    author['publications'] = {v['author_pub_id']:v for v in author['publications']}
    print("âœ… Successfully fetched data for:", name)
    print(json.dumps(author, indent=2)) # åœ¨ Action ä¸­å°å‡ºå¤ªå¤šè³‡è¨Šå¯èƒ½æœƒè®“ Log é›£ä»¥é–±è®€ï¼Œå»ºè­°ç§»é™¤æˆ–è¨»è§£

    # ç¢ºä¿ results è³‡æ–™å¤¾å­˜åœ¨ (æœƒåœ¨ google_scholar_crawler/ ä¸‹å»ºç«‹)
    os.makedirs(results_dir, exist_ok=True)
    print(f"ğŸ“‚ Results directory '{results_dir}' is ready.")

    # --- å°‡çµæœå¯«å…¥æª”æ¡ˆ ---
    gs_data_path = os.path.join(results_dir, 'gs_data.json')
    with open(gs_data_path, 'w', encoding='utf-8') as outfile:
        json.dump(author, outfile, ensure_ascii=False, indent=2)
    print(f"ğŸ“ Wrote gs_data.json to {gs_data_path}")
    
    shieldio_data = {
      "schemaVersion": 1,
      "label": "citations",
      "message": f"{author['citedby']}",
    }
    
    gs_data_shieldsio_path = os.path.join(results_dir, 'gs_data_shieldsio.json')
    with open(gs_data_shieldsio_path, 'w', encoding='utf-8') as outfile:
        json.dump(shieldio_data, outfile, ensure_ascii=False, indent=2)
    print(f"ğŸ“ Wrote gs_data_shieldsio.json to {gs_data_shieldsio_path}")

    print("âœ… Crawler script finished successfully.")

except Exception as e:
    print(f"âŒ An error occurred: {e}")
    exit(1)